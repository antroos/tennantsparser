#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞—É–∫—Ü–∏–æ–Ω–∞ Tennants
"""

import time
import requests
from bs4 import BeautifulSoup
import re
from pathlib import Path
import csv
from datetime import datetime
from urllib.parse import urljoin, urlparse
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class FullAuctionParser:
    def __init__(self, auction_title="", auction_date=""):
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –°–ï–°–°–ò–Ø –° –ü–£–õ–û–ú –°–û–ï–î–ò–ù–ï–ù–ò–ô
        self.session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.3,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=20,  # –ø—É–ª –∏–∑ 20 —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            pool_maxsize=20,
            pool_block=False
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # üî• –°–û–ó–î–ê–ï–ú –£–ù–ò–ö–ê–õ–¨–ù–£–Æ –ü–ê–ü–ö–£ –î–õ–Ø –ü–ê–†–°–ò–ù–ì–ê
        now = datetime.now()
        parsing_time = now.strftime("%Y-%m-%d_%H-%M")
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∞—É–∫—Ü–∏–æ–Ω–∞ –¥–ª—è —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        clean_auction_name = self.clean_filename(auction_title) if auction_title else "Unknown_Auction"
        clean_auction_date = auction_date if auction_date else "Unknown_Date"
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –ø–∞–ø–∫–∏
        folder_name = f"{clean_auction_name}_{clean_auction_date}_parsed_{parsing_time}"
        
        self.working_dir = Path(folder_name)
        self.working_dir.mkdir(exist_ok=True)
        
        self.images_dir = self.working_dir / "images"
        self.images_dir.mkdir(exist_ok=True)
        
        # üî• –°–û–ó–î–ê–ï–ú –ò–ù–§–û–†–ú–ê–¢–ò–í–ù–û–ï –ò–ú–Ø –§–ê–ô–õ–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•
        db_filename = f"{clean_auction_name}_{clean_auction_date}_{parsing_time}.csv"
        self.db_file = self.working_dir / db_filename
        self.init_database()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–ª–µ–π
        self.field_stats = {}
        
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {self.working_dir}")
    
    def clean_filename(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞/–ø–∞–ø–∫–∏"""
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –∑–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        clean = re.sub(r'[^\w\s-]', '', text)
        clean = re.sub(r'[-\s]+', '_', clean)
        return clean[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
    
    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CSV –±–∞–∑—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–æ–ª—è–º–∏"""
        headers = [
            'timestamp',
            'auction_id', 
            'auction_title',
            'auction_date',
            'lot_system_id',
            'lot_number',
            'lot_title',
            'lot_description', 
            'lot_url',
            'image_url',
            'image_high_res_url',
            'additional_images_count',
            'additional_images_urls',
            'lot_estimate',
            'lot_sold_price',
            'lot_status',
            'buyer_premium',
            'condition_report',
            'dimensions',
            'materials',
            'period_dating',
            'artist_maker',
            'origin_country',
            'lot_category',
            'full_lot_info'
        ]
        
        if not self.db_file.exists():
            with open(self.db_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(headers)
    
    def parse_lot_page(self, lot_url):
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ª–æ—Ç–∞"""
        try:
            print(f"üéØ –ü–ê–†–°–ò–ù–ì –õ–û–¢–ê: {lot_url}")
            
            response = self.session.get(lot_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            lot_data = {}
            lot_data['url'] = lot_url
            lot_data['timestamp'] = datetime.now().isoformat()
            
            # Auction ID –∏–∑ URL
            auction_id_match = re.search(r'au=(\d+)', lot_url)
            lot_data['auction_id'] = auction_id_match.group(1) if auction_id_match else ""
            
            # System ID –ª–æ—Ç–∞ –∏–∑ URL
            lot_id_match = re.search(r'lot=(\d+)', lot_url)
            lot_data['lot_system_id'] = lot_id_match.group(1) if lot_id_match else ""
            
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∞—É–∫—Ü–∏–æ–Ω–∞ –∏–∑ breadcrumb
            auction_title = ""
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ù–ê–ó–í–ê–ù–ò–Ø –ê–£–ö–¶–ò–û–ù–ê
            # –°–ø–æ—Å–æ–± 1: –ò—â–µ–º –≤ H4 —Å –∫–ª–∞—Å—Å–æ–º auction-title  
            auction_h4 = soup.find('h4', {'class': 'auction-title'}) or soup.find('H4', {'class': 'auction-title'})
            if auction_h4:
                auction_link = auction_h4.find('a')
                if auction_link:
                    auction_title = auction_link.get_text(strip=True)
                    # –£–±–∏—Ä–∞–µ–º HTML entities
                    auction_title = auction_title.replace('&amp;', '&')
            
            # –°–ø–æ—Å–æ–± 2: –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ auctiondetails tab
            if not auction_title:
                auction_tab = soup.find('div', {'id': 'auctiondetails'})
                if auction_tab:
                    auction_link = auction_tab.find('a', href=re.compile(r'auction/search\?au='))
                    if auction_link:
                        auction_title = auction_link.get_text(strip=True).replace('&amp;', '&')
            
            # –°–ø–æ—Å–æ–± 3: –ü–æ–∏—Å–∫ –≤ breadcrumb (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤)
            if not auction_title:
                breadcrumb = soup.find('ol', {'class': 'breadcrumb'}) 
                if breadcrumb:
                    auction_link = breadcrumb.find('a', href=re.compile(r'auction/details'))
                    if auction_link:
                        auction_title = auction_link.get_text(strip=True)
            
            # –°–ø–æ—Å–æ–± 4: –ü–æ–∏—Å–∫ –≤ —Å–∫—Ä—ã—Ç—ã—Ö –ø–æ–ª—è—Ö —Ñ–æ—Ä–º—ã
            if not auction_title:
                append_text_input = soup.find('input', {'id': 'AppendText'})
                if append_text_input:
                    append_value = append_text_input.get('value', '')
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∞—É–∫—Ü–∏–æ–Ω–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "Lot 1 (Antiques & Interiors, to include...)"
                    match = re.search(r'\(([^,]+,[^)]+)\)', append_value)
                    if match:
                        auction_title = match.group(1).replace('&amp;', '&')
            
            lot_data['auction_title'] = auction_title
            
            # –î–∞—Ç–∞ –∞—É–∫—Ü–∏–æ–Ω–∞ - –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = soup.get_text()
            year_match = re.search(r'20\d{2}', page_text)
            lot_data['auction_date'] = year_match.group(0) if year_match else "2025"
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ù–û–ú–ï–†–ê –õ–û–¢–ê
            lot_number = ""
            # –ò—â–µ–º –≤ span —Å –∫–ª–∞—Å—Å–æ–º lot-number
            lot_number_span = soup.find('span', {'class': 'lot-number'})
            if lot_number_span:
                lot_text = lot_number_span.get_text(strip=True)
                lot_match = re.search(r'Lot\s+(\d+)', lot_text)
                if lot_match:
                    lot_number = lot_match.group(1)
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ H3 —Å –∫–ª–∞—Å—Å–æ–º lot-a-t
            if not lot_number:
                h3_lot = soup.find('h3', {'class': 'lot-a-t'}) or soup.find('H3', {'class': 'lot-a-t'})
                if h3_lot:
                    lot_number = h3_lot.get_text(strip=True)
            
            # –ü–æ–∏—Å–∫ –≤ title —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not lot_number:
                title_tag = soup.find('title')
                if title_tag:
                    title_text = title_tag.get_text(strip=True)
                    lot_match = re.search(r'Lot\s+(\d+)', title_text)
                    if lot_match:
                        lot_number = lot_match.group(1)
            
            lot_data['lot_number'] = lot_number
            lot_data['lot_title'] = lot_number  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–º–µ—Ä –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –û–ü–ò–°–ê–ù–ò–Ø –õ–û–¢–ê  
            lot_description = ""
            # –ò—â–µ–º –≤ div —Å –∫–ª–∞—Å—Å–æ–º lot-desc
            lot_desc_div = soup.find('div', {'class': 'lot-desc'})
            if lot_desc_div:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                paragraphs = lot_desc_div.find_all('p')
                if paragraphs:
                    desc_parts = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text:
                            desc_parts.append(text)
                    lot_description = ' '.join(desc_parts)
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                    lot_description = lot_desc_div.get_text(strip=True)
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ title/meta description
            if not lot_description:
                meta_desc = soup.find('meta', {'name': 'description'})
                if meta_desc:
                    content = meta_desc.get('content', '')
                    # –£–±–∏—Ä–∞–µ–º "Lot X - " –∏–∑ –Ω–∞—á–∞–ª–∞
                    desc_clean = re.sub(r'^Lot\s+\d+\s*[-:]?\s*', '', content)
                    lot_description = desc_clean
            
            lot_data['lot_description'] = lot_description
            
            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –∏—â–µ–º main image
            main_img = soup.find('img', {'id': 'lot-image'}) or soup.find('img', {'class': 'main-image'}) or soup.find('img', src=re.compile(r'stock.*medium'))
            if main_img:
                img_src = main_img.get('src', '')
                if img_src:
                    # –ü–æ–ª–Ω—ã–π URL
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src
                    elif img_src.startswith('/'):
                        img_src = 'https://tennants.blob.core.windows.net' + img_src
                    
                    lot_data['image_url'] = img_src
                    # HD –≤–µ—Ä—Å–∏—è (—É–±–∏—Ä–∞–µ–º -medium)
                    lot_data['image_high_res_url'] = img_src.replace('-medium', '')
                else:
                    lot_data['image_url'] = ""
                    lot_data['image_high_res_url'] = ""
            else:
                lot_data['image_url'] = ""
                lot_data['image_high_res_url'] = ""
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –û–¶–ï–ù–û–ß–ù–û–ô –°–¢–û–ò–ú–û–°–¢–ò
            estimate_text = ""
            # –ò—â–µ–º –≤ div —Å –∫–ª–∞—Å—Å–æ–º estimate
            estimate_div = soup.find('div', {'class': 'estimate'})
            if estimate_div:
                estimate_text = estimate_div.get_text(strip=True)
                # –û—á–∏—â–∞–µ–º –æ—Ç "Estimate" –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                estimate_text = re.sub(r'^Estimate\s*', '', estimate_text, flags=re.IGNORECASE)
                # –ó–∞–º–µ–Ω—è–µ–º HTML entities
                estimate_text = estimate_text.replace('&#163;', '¬£')
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not estimate_text:
                estimate_match = re.search(r'Estimate[:\s]*¬£[\d,\s-]+', page_text, re.IGNORECASE)
                if estimate_match:
                    estimate_text = estimate_match.group(0).replace('Estimate', '').strip(' :')
            
            lot_data['lot_estimate'] = estimate_text
            
            # –¶–µ–Ω–∞ –ø—Ä–æ–¥–∞–∂–∏ –∏ —Å—Ç–∞—Ç—É—Å (–æ–±—ã—á–Ω–æ –ø—É—Å—Ç—ã–µ –¥–ª—è –±—É–¥—É—â–∏—Ö –∞—É–∫—Ü–∏–æ–Ω–æ–≤)
            lot_data['lot_sold_price'] = ""
            lot_data['lot_status'] = ""
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ö–û–ú–ò–°–°–ò–ò –ü–û–ö–£–ü–ê–¢–ï–õ–Ø
            premium_text = ""
            # –ò—â–µ–º –≤ div —Å –∫–ª–∞—Å—Å–æ–º buyers-premium
            premium_div = soup.find('div', {'class': 'buyers-premium'})
            if premium_div:
                premium_full_text = premium_div.get_text(strip=True)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Ü–µ–Ω—Ç
                premium_match = re.search(r'(\d+(?:\.\d+)?)%', premium_full_text)
                if premium_match:
                    premium_text = f"{premium_match.group(1)}%"
                
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not premium_text:
                premium_match = re.search(r'(\d+(?:\.\d+)?)%', page_text)
                premium_text = f"{premium_match.group(1)}%" if premium_match else "22.00%"
            
            lot_data['buyer_premium'] = premium_text
            
            # üî• –£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –û–¢–ß–ï–¢–ê –û –°–û–°–¢–û–Ø–ù–ò–ò
            condition_report = ""
            
            # –ò—â–µ–º –≤ —Ç–∞–±–µ condition
            condition_tab = soup.find('div', {'id': 'condition'})
            if condition_tab:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å —Ç–µ–∫—Å—Ç–æ–º
                condition_paragraphs = condition_tab.find_all('p')
                for p in condition_paragraphs:
                    text = p.get_text(strip=True)
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ disclaimer —Ç–µ–∫—Å—Ç—ã
                    if text and not text.startswith('We are happy to provide') and not text.startswith('We cannot guarantee'):
                        condition_report = text
                        break
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ—Ä–∞–∑—ã
            if not condition_report:
                if "no condition report" in page_text.lower():
                    condition_report = "There is no condition report for this lot. Click the 'Ask a question' button below to request further information."
                elif "condition report" in page_text.lower():
                    condition_match = re.search(r'[^.]*condition report[^.]*\.', page_text, re.IGNORECASE)
                    condition_report = condition_match.group(0) if condition_match else "We are happy to provide Condition Reports to Prospective Buyers, but would welcome your request as soon as possible, preferably at least 48 hours before the Day of Sale."
                else:
                    condition_report = "We are happy to provide Condition Reports to Prospective Buyers, but would welcome your request as soon as possible, preferably at least 48 hours before the Day of Sale."
            
            lot_data['condition_report'] = condition_report
            
            # üî• –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–• –ü–û–õ–ï–ô
            description_full = lot_data.get('lot_description', '')
            
            # –†–∞–∑–º–µ—Ä—ã
            lot_data['dimensions'] = self.extract_dimensions(description_full)
            
            # –ú–∞—Ç–µ—Ä–∏–∞–ª—ã
            lot_data['materials'] = self.extract_materials(description_full)
            
            # –ü–µ—Ä–∏–æ–¥ –∏ –¥–∞—Ç–∏—Ä–æ–≤–∫–∞
            lot_data['period_dating'] = self.extract_period_dating(description_full)
            
            # –•—É–¥–æ–∂–Ω–∏–∫/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å
            lot_data['artist_maker'] = self.extract_artist_maker(description_full)
            
            # –°—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è
            lot_data['origin_country'] = self.extract_origin_country(description_full)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ª–æ—Ç–∞
            lot_data['lot_category'] = self.extract_lot_category(soup)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            additional_images = self.extract_additional_images(soup)
            lot_data['additional_images_count'] = len(additional_images)
            lot_data['additional_images_urls'] = ' | '.join(additional_images) if additional_images else ""
            
            # –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ª–æ—Ç–µ
            full_info = f"Lot {lot_data.get('lot_number', 'N/A')} ({lot_data.get('auction_title', 'Unknown Auction')}, {lot_data.get('auction_date', 'Unknown Date')})\n"
            full_info += lot_data.get('lot_description', '')
            if lot_data.get('lot_estimate'):
                full_info += f"\nEstimate: {lot_data['lot_estimate']}"
            if lot_data.get('dimensions'):
                full_info += f"\nDimensions: {lot_data['dimensions']}"
            if lot_data.get('materials'):
                full_info += f"\nMaterials: {lot_data['materials']}"
            if lot_data.get('period_dating'):
                full_info += f"\nPeriod: {lot_data['period_dating']}"
            lot_data['full_lot_info'] = full_info
            
            # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ù–´–• –î–ê–ù–ù–´–•
            print(f"‚úÖ –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
            important_fields = ['lot_number', 'lot_description', 'lot_estimate', 'buyer_premium']
            for field in important_fields:
                value = lot_data.get(field, '')
                if value:
                    display_value = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                    print(f"   ‚úÖ {field}: {display_value}")
                else:
                    print(f"   ‚ùå {field}: –ü–£–°–¢–û!")
            
            # üî• –ü–û–ö–ê–ó–´–í–ê–ï–ú –ù–û–í–´–ï –ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –ü–û–õ–Ø
            new_fields = ['dimensions', 'materials', 'period_dating', 'artist_maker', 'origin_country', 'lot_category', 'additional_images_count']
            print(f"üî• –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–û–õ–Ø:")
            for field in new_fields:
                value = lot_data.get(field, '')
                if value:
                    display_value = str(value)[:80] + "..." if len(str(value)) > 80 else str(value)
                    print(f"   üéØ {field}: {display_value}")
                else:
                    print(f"   ‚ö™ {field}: -")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            other_fields = ['auction_id', 'lot_system_id', 'auction_title', 'image_url', 'condition_report']
            for field in other_fields:
                value = lot_data.get(field, '')
                if value and len(str(value)) > 100:
                    print(f"   üìã {field}: {str(value)[:50]}...")
                elif value:
                    print(f"   üìã {field}: {value}")
            
            return lot_data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–æ—Ç–∞: {e}")
            return None
    
    def save_lot_data(self, lot_data):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ª–æ—Ç–∞ –≤ CSV"""
        with open(self.db_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            row = [
                lot_data.get('timestamp', ''),
                lot_data.get('auction_id', ''),
                lot_data.get('auction_title', ''),
                lot_data.get('auction_date', ''),
                lot_data.get('lot_system_id', ''),
                lot_data.get('lot_number', ''),
                lot_data.get('lot_title', ''),
                lot_data.get('lot_description', ''),
                lot_data.get('url', ''),
                lot_data.get('image_url', ''),
                lot_data.get('image_high_res_url', ''),
                lot_data.get('additional_images_count', ''),
                lot_data.get('additional_images_urls', ''),
                lot_data.get('lot_estimate', ''),
                lot_data.get('lot_sold_price', ''),
                lot_data.get('lot_status', ''),
                lot_data.get('buyer_premium', ''),
                lot_data.get('condition_report', ''),
                lot_data.get('dimensions', ''),
                lot_data.get('materials', ''),
                lot_data.get('period_dating', ''),
                lot_data.get('artist_maker', ''),
                lot_data.get('origin_country', ''),
                lot_data.get('lot_category', ''),
                lot_data.get('full_lot_info', '')
            ]
            writer.writerow(row)
        
        print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.db_file}")
    
    def download_image(self, image_url, lot_id, lot_number="", lot_description="", is_main=True, image_index=0):
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É –ª–æ—Ç–∞"""
        if not image_url:
            return None
        
        try:
            # üî• –°–û–ó–î–ê–ï–ú –û–¢–î–ï–õ–¨–ù–£–Æ –ü–ê–ü–ö–£ –î–õ–Ø –õ–û–¢–ê
            clean_lot_desc = self.clean_filename(lot_description)
            lot_folder_name = f"Lot_{lot_number}_{clean_lot_desc}" if lot_number else f"Lot_ID_{lot_id}"
            lot_images_dir = self.images_dir / lot_folder_name
            lot_images_dir.mkdir(exist_ok=True)
            
            # üî• –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê: –∫–æ—Ä–æ—Ç–∫–∏–π timeout + stream
            response = self.session.get(image_url, timeout=10, stream=True)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            ext = '.jpg'
            if '.png' in image_url:
                ext = '.png'
            
            # –°–æ–∑–¥–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            if is_main:
                filename = f"lot_{lot_id}_main{ext}"
            else:
                filename = f"lot_{lot_id}_additional_{image_index}{ext}"
            
            filepath = lot_images_dir / filename
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filepath}")
            return str(filepath)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    def download_all_lot_images(self, lot_data):
        """üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–æ—Ç–∞"""
        lot_id = lot_data.get('lot_system_id', '')
        lot_number = lot_data.get('lot_number', '')
        lot_description = lot_data.get('lot_description', '')
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        images_to_download = []
        
        # –û—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        main_image_url = lot_data.get('image_url', '')
        if main_image_url:
            images_to_download.append({
                'url': main_image_url,
                'lot_id': lot_id,
                'lot_number': lot_number,
                'lot_description': lot_description,
                'is_main': True,
                'image_index': 0
            })
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        additional_urls = lot_data.get('additional_images_urls', '')
        if additional_urls:
            urls_list = additional_urls.split(' | ')
            for i, url in enumerate(urls_list, 1):
                if url.strip():
                    images_to_download.append({
                        'url': url.strip(),
                        'lot_id': lot_id,
                        'lot_number': lot_number,
                        'lot_description': lot_description,
                        'is_main': False,
                        'image_index': i
                    })
        
        if not images_to_download:
            print(f"üì∑ –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª–æ—Ç–∞ #{lot_number}")
            return []
        
        # üöÄ –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê
        print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º {len(images_to_download)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª–æ—Ç–∞ #{lot_number}...")
        start_time = time.time()
        downloaded_images = []
        
        with ThreadPoolExecutor(max_workers=6) as executor:
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            future_to_image = {
                executor.submit(
                    self.download_image,
                    img_data['url'],
                    img_data['lot_id'],
                    img_data['lot_number'],
                    img_data['lot_description'],
                    img_data['is_main'],
                    img_data['image_index']
                ): img_data for img_data in images_to_download
            }
            
            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            for future in as_completed(future_to_image):
                try:
                    result = future.result()
                    if result:
                        downloaded_images.append(result)
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        
        download_time = time.time() - start_time
        print(f"üì∑ –°–∫–∞—á–∞–Ω–æ {len(downloaded_images)}/{len(images_to_download)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –ª–æ—Ç–∞ #{lot_number} –∑–∞ {download_time:.1f}—Å")
        return downloaded_images
    
    def validate_lot_data(self, lot_data, lot_number):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–ª–µ–π –ª–æ—Ç–∞"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        required_fields = {
            'auction_id': 'ID –∞—É–∫—Ü–∏–æ–Ω–∞',
            'lot_number': '–ù–æ–º–µ—Ä –ª–æ—Ç–∞',
            'lot_title': '–ù–∞–∑–≤–∞–Ω–∏–µ –ª–æ—Ç–∞',
            'lot_description': '–û–ø–∏—Å–∞–Ω–∏–µ –ª–æ—Ç–∞',
            'lot_estimate': '–û—Ü–µ–Ω–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å',
            'buyer_premium': '–ö–æ–º–∏—Å—Å–∏—è –ø–æ–∫—É–ø–∞—Ç–µ–ª—è',
            'image_url': 'URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è',
            'image_high_res_url': 'URL HD –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è',
            'condition_report': '–û—Ç—á–µ—Ç –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏'
        }
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (–∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏)
        additional_fields = {
            'dimensions': '–†–∞–∑–º–µ—Ä—ã',
            'materials': '–ú–∞—Ç–µ—Ä–∏–∞–ª—ã',
            'period_dating': '–ü–µ—Ä–∏–æ–¥/–¥–∞—Ç–∏—Ä–æ–≤–∫–∞',
            'artist_maker': '–•—É–¥–æ–∂–Ω–∏–∫/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å',
            'origin_country': '–°—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è',
            'lot_category': '–ö–∞—Ç–µ–≥–æ—Ä–∏—è –ª–æ—Ç–∞',
            'additional_images_count': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø. –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π'
        }
        
        # –ü–æ–ª—è –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏ (–∞—É–∫—Ü–∏–æ–Ω –Ω–µ —Å–æ—Å—Ç–æ—è–ª—Å—è)
        optional_fields = {
            'lot_sold_price': '–¶–µ–Ω–∞ –ø—Ä–æ–¥–∞–∂–∏',
            'lot_status': '–°—Ç–∞—Ç—É—Å –ª–æ—Ç–∞'
        }
        
        all_fields = {**required_fields, **additional_fields, **optional_fields}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ
        missing_fields = []
        filled_fields = []
        additional_filled = []
        
        for field, description in all_fields.items():
            value = lot_data.get(field, '')
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–ª—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if field not in self.field_stats:
                self.field_stats[field] = {'filled': 0, 'empty': 0}
            
            if value and str(value).strip():
                if field in required_fields:
                    filled_fields.append(f"‚úÖ {description}")
                elif field in additional_fields:
                    additional_filled.append(f"üéØ {description}")
                else:
                    filled_fields.append(f"‚úÖ {description}")
                self.field_stats[field]['filled'] += 1
            else:
                if field in required_fields:
                    missing_fields.append(f"‚ùå {description}")
                    self.field_stats[field]['empty'] += 1
                elif field in additional_fields:
                    self.field_stats[field]['empty'] += 1
                else:
                    missing_fields.append(f"‚è≥ {description} (–æ–∂–∏–¥–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∞—É–∫—Ü–∏–æ–Ω–∞)")
                    self.field_stats[field]['empty'] += 1
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
        total_required = len(required_fields)
        total_filled = len([f for f in filled_fields if "‚úÖ" in f])
        
        print(f"   üìã –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–µ–π –ª–æ—Ç–∞ #{lot_number}:")
        print(f"      ‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è: {total_filled}/{total_required}")
        
        if additional_filled:
            print(f"      üéØ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {len(additional_filled)}/{len(additional_fields)}")
            for field in additional_filled[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"         {field}")
        
        if any("‚ùå" in field for field in missing_fields):
            print(f"      ‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:")
            for field in missing_fields:
                if "‚ùå" in field:
                    print(f"         {field}")
        
        return len([f for f in missing_fields if "‚ùå" in f]) == 0  # True –µ—Å–ª–∏ –Ω–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
    
    def print_field_statistics(self, total_lots):
        """–ü–µ—á–∞—Ç—å –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–æ–ª—è–º"""
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ü–û–õ–ù–ï–ù–ù–û–°–¢–ò –ü–û–õ–ï–ô:")
        print("="*60)
        
        for field, stats in self.field_stats.items():
            filled = stats['filled']
            empty = stats['empty']
            percentage = (filled / total_lots * 100) if total_lots > 0 else 0
            
            status = "‚úÖ" if percentage >= 95 else "‚ö†Ô∏è" if percentage >= 80 else "‚ùå"
            
            print(f"{status} {field:<20} | {filled:>3}/{total_lots:<3} | {percentage:>5.1f}%")
        
        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–ª—è
        problem_fields = [field for field, stats in self.field_stats.items() 
                         if stats['filled'] / total_lots < 0.95 and field not in ['lot_sold_price', 'lot_status']]
        
        if problem_fields:
            print(f"\n‚ö†Ô∏è –ü–û–õ–Ø –° –ù–ò–ó–ö–û–ô –ó–ê–ü–û–õ–ù–ï–ù–ù–û–°–¢–¨–Æ:")
            for field in problem_fields:
                stats = self.field_stats[field]
                percentage = stats['filled'] / total_lots * 100
                print(f"   - {field}: {percentage:.1f}% (–ø—Ä–æ–±–ª–µ–º–∞ –≤ {stats['empty']} –ª–æ—Ç–∞—Ö)")
        else:
            print(f"\nüéâ –í–°–ï –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–û–õ–Ø –ó–ê–ü–û–õ–ù–Ø–Æ–¢–°–Ø –ö–û–†–†–ï–ö–¢–ù–û!")
        
    def get_all_auction_lots(self, auction_url):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤ –∏–∑ –∞—É–∫—Ü–∏–æ–Ω–∞"""
        print(f"üîç –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ê–£–ö–¶–ò–û–ù–ê: {auction_url}")
        print("="*60)
        
        try:
            response = self.session.get(auction_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            lots = []
            seen_lots = set()
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –ª–æ—Ç–æ–≤
            lot_links = soup.find_all('a', href=re.compile(r'/auction/lot/'))
            
            print(f"üîó –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ –ª–æ—Ç–æ–≤: {len(lot_links)}")
            
            for link in lot_links:
                try:
                    lot_url = link.get('href')
                    if not lot_url.startswith('http'):
                        lot_url = 'https://auctions.tennants.co.uk' + lot_url
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –ª–æ—Ç–∞
                    lot_match = re.search(r'lot=(\d+)', lot_url)
                    if not lot_match:
                        continue
                    
                    lot_id = lot_match.group(1)
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                    if lot_id in seen_lots:
                        continue
                    seen_lots.add(lot_id)
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –ª–æ—Ç–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                    lot_text = link.get_text(strip=True)
                    
                    lot_info = {
                        'id': lot_id,
                        'url': lot_url,
                        'preview_text': lot_text
                    }
                    
                    lots.append(lot_info)
                    
                except Exception as e:
                    continue
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª–æ—Ç–æ–≤: {len(lots)}")
            return lots
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∞—É–∫—Ü–∏–æ–Ω–∞: {e}")
            return []
    
    def parse_auction(self, auction_url, max_lots=None, delay=2):
        """–ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ –∞—É–∫—Ü–∏–æ–Ω–∞"""
        print(f"üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ü–ê–†–°–ò–ù–ì –ü–û–õ–ù–û–ì–û –ê–£–ö–¶–ò–û–ù–ê")
        print("="*60)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ª–æ—Ç—ã
        lots = self.get_all_auction_lots(auction_url)
        
        if not lots:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ª–æ—Ç—ã –≤ –∞—É–∫—Ü–∏–æ–Ω–µ")
            return False
        
        total_lots = len(lots)
        if max_lots:
            lots = lots[:max_lots]
            print(f"üéØ –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–æ {max_lots} –ª–æ—Ç–æ–≤ –∏–∑ {total_lots}")
        
        print(f"\nüì¶ –ù–ê–ß–ò–ù–ê–ï–ú –ü–ê–†–°–ò–ù–ì {len(lots)} –õ–û–¢–û–í")
        print("="*50)
        
        success_count = 0
        error_count = 0
        
        for i, lot in enumerate(lots, 1):
            try:
                print(f"\n[{i}/{len(lots)}] –ü–∞—Ä—Å–∏–º –ª–æ—Ç ID: {lot['id']}")
                print(f"URL: {lot['url']}")
                
                # –ü–∞—Ä—Å–∏–º –ª–æ—Ç
                lot_data = self.parse_lot_page(lot['url'])
                
                if lot_data:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
                    self.save_lot_data(lot_data)
                    
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    self.download_all_lot_images(lot_data)
                    
                    success_count += 1
                    lot_number = lot_data.get('lot_number', lot['id'])
                    print(f"‚úÖ –õ–æ—Ç #{lot_number} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    desc = lot_data.get('lot_description', '')
                    if len(desc) > 100:
                        desc = desc[:100] + "..."
                    print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {desc}")
                    print(f"   –û—Ü–µ–Ω–∫–∞: {lot_data.get('lot_estimate', 'N/A')}")
                    
                    # üîç –ü–†–û–í–ï–†–Ø–ï–ú –ó–ê–ü–û–õ–ù–ï–ù–ù–û–°–¢–¨ –ü–û–õ–ï–ô
                    is_valid = self.validate_lot_data(lot_data, lot_number)
                    if not is_valid:
                        print(f"   ‚ö†Ô∏è –õ–æ—Ç #{lot_number} –∏–º–µ–µ—Ç –Ω–µ–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è!")
                    
                else:
                    error_count += 1
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ª–æ—Ç–∞ {lot['id']}")
                
                # –ü—Ä–æ–≥—Ä–µ—Å—Å
                if i % 10 == 0:
                    print(f"\nüìä –ü–†–û–ì–†–ï–°–°: {i}/{len(lots)} ({i/len(lots)*100:.1f}%)")
                    print(f"   –£—Å–ø–µ—à–Ω–æ: {success_count}")
                    print(f"   –û—à–∏–±–æ–∫: {error_count}")
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                if i < len(lots):
                    time.sleep(delay)
                    
            except KeyboardInterrupt:
                print(f"\n‚ö†Ô∏è –ü–†–ï–†–´–í–ê–ù–ò–ï –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú")
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}/{len(lots)} –ª–æ—Ç–æ–≤")
                break
                
            except Exception as e:
                error_count += 1
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è –ª–æ—Ç–∞ {lot['id']}: {e}")
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        print("="*40)
        print(f"–í—Å–µ–≥–æ –ª–æ—Ç–æ–≤ –≤ –∞—É–∫—Ü–∏–æ–Ω–µ: {total_lots}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count + error_count}/{len(lots)}")
        print(f"–£—Å–ø–µ—à–Ω–æ: {success_count}")
        print(f"–û—à–∏–±–æ–∫: {error_count}")
        print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_count/len(lots)*100:.1f}%")
        print(f"üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.working_dir}")
        
        # üìä –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–ª–µ–π
        if success_count > 0:
            self.print_field_statistics(success_count)
        
        return success_count > 0

    def extract_dimensions(self, description_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è"""
        dimensions = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–º–µ—Ä–æ–≤
        patterns = [
            r'(\d+(?:\.\d+)?)\s*cm\s+(?:high|height|h)\b',
            r'(\d+(?:\.\d+)?)\s*cm\s+(?:wide|width|w)\b', 
            r'(\d+(?:\.\d+)?)\s*cm\s+(?:deep|depth|d)\b',
            r'(\d+(?:\.\d+)?)\s*cm\s+(?:long|length|l)\b',
            r'(\d+(?:\.\d+)?)\s*cm\s+(?:diameter|diam)\b',
            r'(\d+(?:\.\d+)?)\s*x\s*(\d+(?:\.\d+)?)\s*(?:x\s*(\d+(?:\.\d+)?))?\s*cm',
            r'(\d+(?:\.\d+)?)\s*inches?\s+(?:high|wide|deep|long)',
            r'(\d+(?:\.\d+)?)\s*"\s+(?:high|wide|deep|long)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, description_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    # –î–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10x20x30 cm)
                    dim_str = ' x '.join([d for d in match if d])
                    dimensions.append(dim_str + ' cm')
                else:
                    dimensions.append(f"{match} cm")
        
        return '; '.join(dimensions) if dimensions else ""
    
    def extract_materials(self, description_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è"""
        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –≤ –∞—É–∫—Ü–∏–æ–Ω–∞—Ö
        materials = [
            'brass', 'bronze', 'copper', 'silver', 'gold', 'platinum',
            'wood', 'oak', 'mahogany', 'walnut', 'pine', 'teak', 'ebony',
            'glass', 'crystal', 'ceramic', 'porcelain', 'earthenware', 'stoneware', 'jasper',
            'marble', 'stone', 'granite', 'slate',
            'fabric', 'silk', 'cotton', 'wool', 'linen', 'velvet', 'leather',
            'plastic', 'resin', 'bakelite',
            'ivory', 'bone', 'mother of pearl',
            'enamel', 'lacquer', 'gilt', 'gilded'
        ]
        
        found_materials = []
        text_lower = description_text.lower()
        
        for material in materials:
            if material in text_lower:
                found_materials.append(material.title())
        
        return ', '.join(found_materials) if found_materials else ""
    
    def extract_period_dating(self, description_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ –∏ –¥–∞—Ç–∏—Ä–æ–≤–∫–∏"""
        periods = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≤–µ–∫–æ–≤
        century_patterns = [
            r'(\d+)(?:st|nd|rd|th)\s+century',
            r'(\d+)(?:st|nd|rd|th)\s+c\.',
        ]
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞—Ç
        date_patterns = [
            r'circa\s+(\d{4})',
            r'c\.\s*(\d{4})',
            r'\b(\d{4})\b',
            r'(\d{4})\s*-\s*(\d{4})',
        ]
        
        for pattern in century_patterns:
            matches = re.findall(pattern, description_text, re.IGNORECASE)
            for match in matches:
                periods.append(f"{match} century")
        
        for pattern in date_patterns:
            matches = re.findall(pattern, description_text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    periods.append(f"{match[0]}-{match[1]}")
                else:
                    periods.append(match)
        
        return ', '.join(periods) if periods else ""
    
    def extract_artist_maker(self, description_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω —Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π"""
        makers = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–π/—Ö—É–¥–æ–∂–Ω–∏–∫–æ–≤
        patterns = [
            r'by\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
            r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*),\s+(?:Paris|London|Berlin|Vienna)',
            r'(?:signed|attributed to|after)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, description_text)
            makers.extend(matches)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_makers = list(dict.fromkeys(makers))
        return ', '.join(unique_makers) if unique_makers else ""
    
    def extract_origin_country(self, description_text):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –ø—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏—è"""
        countries = [
            'French', 'English', 'British', 'German', 'Italian', 'Spanish', 
            'Chinese', 'Japanese', 'American', 'Austrian', 'Dutch', 'Belgian',
            'Russian', 'Scandinavian', 'European'
        ]
        
        text_words = description_text.split()
        found_countries = []
        
        for country in countries:
            if country in text_words:
                found_countries.append(country)
        
        return ', '.join(found_countries) if found_countries else ""
    
    def extract_additional_images(self, soup):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ª–æ—Ç–∞"""
        additional_images = []
        
        # –ò—â–µ–º –≤ condition report
        condition_tab = soup.find('div', {'id': 'condition'})
        if condition_tab:
            condition_images = condition_tab.find_all('img')
            for img in condition_images:
                src = img.get('src', '')
                if src and 'stock' in src:
                    # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Å–æ–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
                    high_res_url = src.replace('-small', '').replace('-medium', '')
                    if high_res_url.startswith('//'):
                        high_res_url = 'https:' + high_res_url
                    elif high_res_url.startswith('/'):
                        high_res_url = 'https://tennants.blob.core.windows.net' + high_res_url
                    additional_images.append(high_res_url)
        
        return additional_images
    
    def extract_lot_category(self, soup):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ª–æ—Ç–∞"""
        # –ò—â–µ–º –≤ h1 —Å –∫–ª–∞—Å—Å–æ–º lot-title
        h1_tag = soup.find('h1', {'class': re.compile(r'lot-title.*cat-\d+')})
        if h1_tag:
            class_attr = h1_tag.get('class', [])
            for cls in class_attr:
                if cls.startswith('cat-'):
                    # –ú–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å –∫–∞—Ç–µ–≥–æ—Ä–∏–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    return cls
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ –∏—â–µ–º –≤ option elements
        options = soup.find_all('option')
        for option in options:
            text = option.get_text(strip=True)
            if any(keyword in text.lower() for keyword in ['ceramics', 'glass', 'furniture', 'art', 'jewelry']):
                return text
        
        return ""

def main():
    # URL –∞—É–∫—Ü–∏–æ–Ω–∞, –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ä–∞–Ω–µ–µ
    auction_url = "https://auctions.tennants.co.uk/auction/details/180725-antiques--interiors-to-include-designer-fashion-and-affordable-modern--contemporary-art/?au=14251"
    
    print("üéØ –ü–ê–†–°–ò–ù–ì –ê–£–ö–¶–ò–û–ù–ê: Antiques & Interiors (18 –∏—é–ª—è 2025)")
    print("="*70)
    
    # üî• –ü–û–õ–£–ß–ê–ï–ú –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û–ë –ê–£–ö–¶–ò–û–ù–ï –î–õ–Ø –ù–ê–ó–í–ê–ù–ò–Ø –ü–ê–ü–ö–ò
    print("üìã –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞—É–∫—Ü–∏–æ–Ω–µ...")
    
    # –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∞—É–∫—Ü–∏–æ–Ω–∞
    try:
        import requests
        from bs4 import BeautifulSoup
        
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        response = session.get(auction_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∞—É–∫—Ü–∏–æ–Ω–∞
        auction_title = ""
        title_tag = soup.find('title')
        if title_tag:
            full_title = title_tag.get_text(strip=True)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ " - "
            auction_title = full_title.split(' - ')[0] if ' - ' in full_title else full_title
        
        if not auction_title:
            auction_title = "Antiques & Interiors"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É –∞—É–∫—Ü–∏–æ–Ω–∞
        auction_date = "2025-07-18"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        date_element = soup.find('p', {'class': 'date-title'})
        if date_element:
            date_text = date_element.get_text(strip=True)
            # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ "18th Jul, 2025"
            date_match = re.search(r'(\d+)\w+\s+(\w+),?\s+(\d{4})', date_text)
            if date_match:
                day, month, year = date_match.groups()
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Å—è—Ü
                month_map = {
                    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
                    'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08', 
                    'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
                }
                month_num = month_map.get(month, '07')
                auction_date = f"{year}-{month_num}-{day.zfill(2)}"
        
        print(f"‚úÖ –ù–∞–∑–≤–∞–Ω–∏–µ –∞—É–∫—Ü–∏–æ–Ω–∞: {auction_title}")
        print(f"‚úÖ –î–∞—Ç–∞ –∞—É–∫—Ü–∏–æ–Ω–∞: {auction_date}")
        
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞—É–∫—Ü–∏–æ–Ω–µ: {e}")
        auction_title = "Antiques_Interiors"
        auction_date = "2025-07-18"
    
    # üî• –°–û–ó–î–ê–ï–ú –ü–ê–†–°–ï–† –° –ò–ù–§–û–†–ú–ê–¶–ò–ï–ô –û–ë –ê–£–ö–¶–ò–û–ù–ï
    parser = FullAuctionParser(auction_title=auction_title, auction_date=auction_date)
    
    # üîç –ü–û–î–°–ß–ò–¢–´–í–ê–ï–ú –ö–û–õ–ò–ß–ï–°–¢–í–û –õ–û–¢–û–í –í –ê–£–ö–¶–ò–û–ù–ï
    print("\nüìä –ü–æ–¥—Å—á–µ—Ç –ª–æ—Ç–æ–≤ –≤ –∞—É–∫—Ü–∏–æ–Ω–µ...")
    try:
        lots = parser.get_all_auction_lots(auction_url)
        total_lots_count = len(lots) if lots else 0
        
        if total_lots_count > 0:
            print(f"‚úÖ –í—Å–µ–≥–æ –ª–æ—Ç–æ–≤ –≤ –∞—É–∫—Ü–∏–æ–Ω–µ: {total_lots_count}")
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –ª–æ—Ç—ã –≤ –∞—É–∫—Ü–∏–æ–Ω–µ")
            return
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ –ª–æ—Ç–æ–≤: {e}")
        total_lots_count = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ª–æ—Ç–æ–≤
    try:
        user_input = input(f"\nü§î –°–∫–æ–ª—å–∫–æ –ª–æ—Ç–æ–≤ –ø–∞—Ä—Å–∏—Ç—å –∏–∑ {total_lots_count}? (Enter = –≤—Å–µ –ª–æ—Ç—ã, —á–∏—Å–ª–æ = –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å): ").strip()
        max_lots = None
        if user_input and user_input.isdigit():
            max_lots = int(user_input)
            print(f"‚úÖ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å –º–∞–∫—Å–∏–º—É–º {max_lots} –ª–æ—Ç–æ–≤ –∏–∑ {total_lots_count}")
        else:
            print(f"‚úÖ –ë—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å –í–°–ï {total_lots_count} –ª–æ—Ç–æ–≤ –∞—É–∫—Ü–∏–æ–Ω–∞")
    except KeyboardInterrupt:
        print("\n‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
    success = parser.parse_auction(auction_url, max_lots=max_lots)
    
    if success:
        print(f"\nüéâ –ü–ê–†–°–ò–ù–ì –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {parser.working_dir}")
        print(f"üíæ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {parser.db_file}")
        print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã –ø–æ –ø–∞–ø–∫–∞–º –ª–æ—Ç–æ–≤ –≤: {parser.images_dir}")
    else:
        print(f"\n‚ùå –ü–ê–†–°–ò–ù–ì –ù–ï –£–î–ê–õ–°–Ø")

if __name__ == "__main__":
    main() 